#!/usr/bin/env python3

import sys
sys.path.append("/usr/local/lib/python3.9/site-packages/")

################# WARNING ##################
# FOR NOW I'M DISABLING THE HTTPS CONNCECTION WARNING, BUT THIS CAN HIDE SOME REAL HTTPS ISSUES!!!
import urllib3
import warnings

# Suppress only the InsecureRequestWarning from urllib3
warnings.filterwarnings('ignore', message='Unverified HTTPS request', category=urllib3.exceptions.InsecureRequestWarning)
############################################

#import requests
#import certifi
import argparse
import argcomplete
import configparser
import json
import ast
import re
import os
import numpy as np

# Include to compute the sha-256 checksum of the data file
import hashlib

### Set Rucio virtual environment configuration ###
#os.environ['RUCIO_HOME']=os.path.expanduser('~/Rucio-v2/rucio')
from rucio.client.client import Client
from rucio.client.didclient import DIDClient
from rucio.client.uploadclient import UploadClient
from rucio.client.downloadclient import DownloadClient
#import rucio.rse.rsemanager as rsemgr
from rucio.client.ruleclient import RuleClient
from rucio.common import exception #import (AccountNotFound, Duplicate, RucioException, DuplicateRule, InvalidObject, DataIdentifierAlreadyExists, FileAlreadyExists, RucioException,
                                    #AccessDenied, InsufficientAccountLimit, RuleNotFound, AccessDenied, InvalidRSEExpression,
                                    #InvalidReplicationRule, RucioException, DataIdentifierNotFound, InsufficientTargetRSEs,
                                    #ReplicationRuleCreationTemporaryFailed, InvalidRuleWeight, StagingAreaRuleRequiresLifetime)

#from rucio.common.utils import adler32, detect_client_location, execute, generate_uuid, md5, send_trace, GLOBALLY_SUPPORTED_CHECKSUMS

def compute_sha256(file_path):
    hash_sha256 = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(), b""):
            hash_sha256.update(chunk)
    return hash_sha256.hexdigest()

# Function to map operators to corresponding SQL-friendly keys
def get_operator_key(field, operator):
    if operator != '=':
        operator_map = {
            '>=': 'gte',
            '<=': 'lte',
            '>': 'gt',
            '<': 'lt',
            '!=': 'ne',
        }
        return f"{field}.{operator_map[operator]}"
    else:
        return f"{field}"

# Function to parse filters from string with logical operators AND, OR to a list of dicts as in rucio's filter engine
def parse_filters(input_str):
    # Split by WORD "OR", to create separate dicts for OR conditions, avoiding splitting words like "ORIGINATOR"
    or_conditions = re.split(r'\bOR\b', input_str)
    filters = []

    for or_cond in or_conditions:
        # Same as for the "OR"
        and_conditions = re.split(r'\bAND\b', or_cond)
        and_dict = {}
        
        for cond in and_conditions:
            # Regex to capture key, operator, and value
            match = re.match(r'(\w+)\s*(>=|<=|!=|>|<|=)\s*([^\s]+)', cond.strip())
            
            if match:
                field, operator, value = match.groups()
                operator_key = get_operator_key(field, operator)
                and_dict[operator_key] = value.strip()
        
        filters.append(and_dict)

    return filters

class IDL():
    def __init__(self):
        #self.scope = scope
        #self.rse = rse
        #self.working_folder = working_folder

        self.didc = DIDClient()
        self.uplc = UploadClient()
        self.dwnc = DownloadClient()
        #self.rulesClient = RuleClient()
        
        # Read account from the rucio.cfg in /opt/conda/etc/rucio.cfg. Edit it if your rucio config file is somewhere else
        config = configparser.ConfigParser()

        try:
            if os.path.exists("/opt/conda/etc/rucio.cfg"):
                config.read('/opt/conda/etc/rucio.cfg')
                account = config.get('client', 'account')
            elif os.path.exists("/opt/rucio/etc/rucio.cfg"):
                config.read('/opt/rucio/etc/rucio.cfg')
                account = config.get('client', 'account')
        except FileNotFoundError as e:
            print(f"ERROR: {e}")
        
        # Configuration
        self.account = account

        # account=account
        self.client = Client(account=account)

    def add_dataset(self, scope, name, statuses, meta, files, rse):
        pass

    def add_container(self, scope, name, statuses, meta):
        pass

    def customUpload(self, scope, file, meta, rse, bulk):
        '''
        Custom upload: upload file + set-metadata from .json file

        :param file: file path of the data file
        :param meta: file path of the .json metadata file
        '''
        if bulk == False:
            did_name = file.split('/')[-1] # Use the file name as the DID name
            did_scope = scope
            try:
                # Upload the file
                self.uplc.upload([{
                    'path': file,
                    'rse': rse,
                    'did_scope': did_scope,
                    'did_name': did_name,  
                }])

                # Parse the JSON file in a Python dict and add the "DID" and "sha-256" metadata
                with open(meta, 'r') as m:
                    json_dict = json.load(m)
                    json_dict['LINK'] = f'{did_scope}:{did_name}'
                    json_dict['sha256'] = compute_sha256(file)
                    json_string = json.dumps(json_dict)

                #Â Set the metadata for the uploaded file
                self.didc.set_metadata(scope=did_scope, name=did_name, key='JSON', value=json_string)
                
                #did_name = file.split('/')[-1]
                #for key, value in json_dict.items():
                #    did_name = file.split('/')[-1]
                #    self.didc.set_metadata(scope=scope, name=did_name, key=key, value=value)

            except Exception as e:
                print(f"Error: {e}")
        elif bulk == True:
            self.customUploadBulk(scope=did_scope, rse=rse, files=files, metas=metas)

    def customUploadBulk(self, scope, files, metas, rse):
        pass
    
    def customDownload(self, scope, file, base_dir, no_subdir, bulk):
        if bulk == False:
            name = file 
            # try: 
            #     name = file.split('/')[-1] # Use the file name as the DID name
            # except:
            #     name = file
            did = f"{scope}:{name}"
            dwn_file = self.dwnc.download_dids(items=[{'did': did, 'base_dir': base_dir, 'no_subdir': no_subdir}])
            # I'm printing the dwn_file dict. The file is downloaded automatically
            max_key_length = max(len(key) for key in dwn_file[0].keys())
            for keys, values in dwn_file[0].items():
                print(f"{keys}:".ljust(max_key_length + 1) + f"  {values}")
        elif bulk == True:
            self.customDownloadBulk(items=[{'did': did, 'base_dir': base_dir, 'no_subdir': no_subdir}])

    def customDownloadBulk(self, items):
        pass

    def customSetMeta(self, scope, file, meta):
        '''
        Custom set-metadata: set-metadata from .json file if you need to edit the metadata of a DID

        :param file: file path of the data file
        :param meta: file path of the .json metadata file
        '''
        did_name = file.split('/')[-1] # Use the file name as the DID name
        did_scope = scope
        try:
            # Parse the JSON file in a Python dict and add the "DID" and "sha-256" metadata
            with open(meta, 'r') as m:
                json_dict = json.load(m)
                json_dict['LINK'] = f'{did_scope}:{did_name}'
                json_dict['sha256'] = compute_sha256(file)
                json_string = json.dumps(json_dict)

            #Â Set the metadata for the file data
            self.didc.set_metadata(scope=did_scope, name=did_name, key='JSON', value=json_string)

        except Exception as e:
            print(f"Error: {e}")
    
    def customGetMeta(self, scope, name, plugin):
        '''
        Custom get-metadata: get-metadata of the specific DID for the custom plugin "IDL"

        :param file: file path of the data file
        :param meta: file path of the .json metadata file
        '''
        if plugin == "IDL":
            # Download of the file in /tmp/ without a subdir
            tmp_file = self.dwnc.download_dids(items=[{'did': f'{scope}:{name}', 'base_dir': '/tmp/', 'no_subdir': True}])
            hash_data = compute_sha256(tmp_file[0]['temp_file_path'][:-5])
            name = hash_data + ':' + name
            # After it is being used it is removed from /tmp/. You MUST have your donwloaded files in another directory!
            if os.path.exists(tmp_file[0]['temp_file_path'][:-5]):
                os.remove(tmp_file[0]['temp_file_path'][:-5])
            try:
                #Â Get the metadata for the specific DID
                dict = self.didc.get_metadata(plugin = "IDL", scope=scope, name=name)
                # Print the resulting dict in the same format as the rucio get-metadata
                max_key_length = max(len(key) for key in dict.keys())
                for keys, values in dict.items():
                    print(f"{keys}:".ljust(max_key_length + 1) + f"  {values}")

            except Exception as e:
                print(f"Error: {e}")
        else:
            try:
                dict = self.didc.get_metadata(plugin = "DID_COLUMN", scope=scope, name=name)
                # Print the resulting dict in the same format as the rucio get-metadata
                max_key_length = max(len(key) for key in dict.keys())
                for keys, values in dict.items():
                    print(f"{keys}:".ljust(max_key_length + 1) + f"  {values}")
            except Exception as e:
                print(f"Error: {e}")

    
    def customListDids(self, scope, filters):
        #did_name = file.split('/')[-1] # Use the file name as the DID name
        did_scope = scope
        try:
            filters = parse_filters(filters)
            for fil in filters:
                for key, value in fil.items():
                    if key.startswith("EPOCH") or key.startswith("CREATION_DATE"):
                        value = np.datetime64(f'{value}')
            i = 1
            # For now, I'm passing the SELECTs to the plugin via the filters
            select_dict = {'sql_select': 'LINK'}
            filters.append(select_dict)
            res = list(self.didc.list_dids(scope=did_scope, filters=filters, did_type='all', long=False, recursive=False))
            max_list_len = len(str(abs(len(res))))
            for d in res:
                #for key in d.keys():
                #    if key == 'LINK':
                #        d['LINK'] = d['LINK'][:-1]
                print(f"{i}:".ljust(max_list_len + 1) + f"{json.dumps(d, indent=4)}\n")
                i = i + 1
        except Exception as e:
            print(f"Error: {e}")

    def customQuery(self, scope, select, filters):
        #did_name = file.split('/')[-1] # Use the file name as the DID name
        did_scope = scope
        try:
            filters = parse_filters(filters)
            for fil in filters:
                for key, value in fil.items():
                    if key.startswith("EPOCH") or key.startswith("CREATION_DATE"):
                        value = np.datetime64(f'{value}')
            i = 1
            # For now, I'm passing the SELECTs to the plugin via the filters
            select_dict = {'sql_select': select + ', LINK'}
            filters.append(select_dict)
            res = list(self.didc.list_dids(scope=did_scope, filters=filters, did_type='all', long=False, recursive=False))
            max_list_len = len(str(abs(len(res))))
            for d in res:
                if "EPOCH" in d:
                    value = d["EPOCH"]
                    d["EPOCH"] = d["EPOCH"].strftime("%Y-%m-%dT%H:%M:%S") + f".{value.microsecond:06d}"
                if "CREATION_DATE" in d:
                    d["CREATION_DATE"] = d["CREATION_DATE"].replace(' ', 'T')

                # for key in d.keys():
                #     if key == 'LINK':
                #         d['LINK'] = d['LINK'][:-1]
                print(f"{i}:".ljust(max_list_len + 1) + f"{json.dumps(d, indent=4)}\n")
                i = i + 1
        except Exception as e:
            print(f"Error: {e}")
            

def main():
    # Setup argument parser
    parser = argparse.ArgumentParser(description="IDL client: upload (+ set-metadata), get-metadata and SQL queries with custom plugin") # This is the description of the binary if you read the help message {-h, --help}
    
    # Subparsers
    subpars = parser.add_subparsers(dest="method", help="Available methods")

    # Subparser upload
    pars_upload = subpars.add_parser("upload", help="Upload and set-metadata of a file")
    # Upload's args and flags
    pars_upload.add_argument("--scope", type=str, required=True, help="Scope")
    pars_upload.add_argument("--rse", type=str, required=True, help="RSE expression")
    pars_upload.add_argument("--file", type=str, required=True, help="File path of the data file")
    pars_upload.add_argument("--meta", type=str, required=True, help="File path of the metadata file")
    pars_upload.add_argument("--bulk", type=bool, default=False, help="Upload a list of files. Default: False [TO BE IMPLEMENTED]")

    # Subparser download
    pars_download = subpars.add_parser("download", help="Download a DID")
    # Download's args and flags
    pars_download.add_argument("--scope", type=str, required=True, help="Scope")
    pars_download.add_argument("--file", type=str, required=True, help="File path of the data file")
    pars_download.add_argument("--base_dir", type=str, default=os.path.expanduser("~"), help="Base directory where the downloaded files will be stored. Default: '~'")
    pars_download.add_argument("--no_subdir", type=bool, default=False, help="If true, files are written directly into base_dir. Default: False")
    pars_download.add_argument("--bulk", type=bool, default=False, help="Download a list of files. Default: False [TO BE IMPLEMENTED]")

    # Subparser get-metadata
    pars_get = subpars.add_parser("get-metadata", help="Get-metadata of a file")
    # Get's args and flags
    pars_get.add_argument("--plugin", choices=['IDL', 'DID_COLUMN'], default="IDL", type=str, help="Plugin to use. Default: 'IDL'")
    pars_get.add_argument("--scope", type=str, required=True, help="Scope")
    pars_get.add_argument("--name", type=str, required=True, help="Name of the DID")

    # Subparser list-dids
    pars_list = subpars.add_parser("list-dids", help="List of DIDs satisfying the filters")
    # List-dids' args and flags
    pars_list.add_argument("--scope", type=str, required=True, help="Scope")
    pars_list.add_argument("--filters", type=str, help="Filters to retrieve the list of DIDs satisfying them. Operators must belong to the set of (<=, >=, =, !=, >, <) and the logical expressions AND and OR can be used")

    # Subparser sql query
    pars_sql = subpars.add_parser("sql", help="List of DIDs satisfying the filters, each one in a dictionary where the key are the SELECTs")
    # Get's args and flags
    pars_sql.add_argument("--scope", type=str, required=True, help="Scope")
    pars_sql.add_argument("--filters", type=str, required=True, help="Filters to retrieve the list of DIDs satisfying them. Operators must belong to the set of (<=, >=, =, !=, >, <) and the logical expressions AND and OR can be used")
    pars_sql.add_argument("--select", type=str, help="SELECTs for SQL queries, e.g. --select 'PARTICIPANT_1, EPOCH'. The LINK, which is the DID, is always present in the selects")

    # Subparser add-dataset
    pars_add_dataset = subpars.add_parser("add-dataset", help="Add a dataset")
    # Add-collection's args and flags
    pars_add_dataset.add_argument("--scope", type=str, required=True, help="Scope")
    pars_add_dataset.add_argument("--name", type=str, required=True, help="Name of the dataset")
    pars_add_dataset.add_argument("--statuses", type=str, help="Dictionary with statuses, e.g. {'monotonic':True}")
    pars_add_dataset.add_argument("--meta", type=str, help="'DID_COLUMN' metadata of the dataset")
    pars_add_dataset.add_argument("--files", type=str, help="Content of the dataset. List of file paths")
    pars_add_dataset.add_argument("--rse", type=str, help="RSE expression")

    # Subparser add-container
    pars_add_container = subpars.add_parser("add-container", help="Add a container")
    # Add-collection's args and flags
    pars_add_container.add_argument("--scope", type=str, required=True, help="Scope")
    pars_add_container.add_argument("--name", type=str, required=True, help="Name of the container")
    pars_add_container.add_argument("--statuses", type=str, help="Dictionary with statuses, e.g. {'monotonic':True}")
    pars_add_container.add_argument("--meta", type=str, help="'DID_COLUMN' metadata of the container")

    # Enable autocomplete
    argcomplete.autocomplete(parser)

    # Parse arguments
    args = parser.parse_args()

    # Create an instance of IDL
    myClass = IDL()

    # Call the chosen method
    if args.method == 'add-dataset':
        myClass.add_dataset(args.scope, args.name, args.rse, args.statuses, args.meta, args.files)
    elif args.method == 'add-container':
        myClass.add_dataset(args.scope, args.name, args.statuses, args.meta)
    elif args.method == 'upload':
        myClass.customUpload(args.scope, args.file, args.meta, args.rse, args.bulk)
    elif args.method == 'download':
        myClass.customDownload(args.scope, args.file, args.base_dir, args.no_subdir, args.bulk)
    #elif args.method == 'set':
    #    myClass.customSetMeta(args.scope, args.file, args.meta)
    elif args.method == 'get-metadata':
        myClass.customGetMeta(args.scope, args.name, args.plugin) 
    elif args.method == 'list-dids':
        myClass.customListDids(args.scope, args.filters) #ast.literal_eval(args.filters))
    elif args.method == 'sql':
        myClass.customQuery(args.scope, args.select, args.filters)
    else:
        # If no method is selected, print the general help
        parser.print_help()


if __name__ == "__main__":
    # Make a request to the service behind the Ingress
    #response = requests.get('https://rucio-server.131.154.98.24.myip.cloud.infn.it:443', verify=certifi.where()) #verify='/etc/pki/tls/certs/ca-bundle.crt'       
    
    main()
