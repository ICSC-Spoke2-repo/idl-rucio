# IG-IDL project: Rucio Data Lake

- [Preliminaries](#preliminaries)
- [RUCIO Server Deployment](#rucio-server-deployment)
- [S3 Storage Endpoint](#s3-storage-endpoint)
  - [RSE Setup](#rse-setup)
  - [RSE Configuration](#rse-configuration)
  - [RSE Test](#rse-test)
- [IDL-dedicated components](#idl-dedicated-components)
  - [K8s Cronjobs: Periodic Dump](#k8s-cronjobs-periodic-dump)
  - [K8s Cronjobs: Consistency-check](#k8s-cronjobs-consistency-check)
  - [Shared Storage: NFS](#shared-storage-nfs)
    - [NFS Server Setup](#nfs-server-setup)
    - [RUCIO Server's PVC](#rucio-servers-pvc)
- [Next-Gen RUCIO WebUI](#next-gen-rucio-webui)
- [JupyterHub instance](#jupyterhub-instance)
- [Extra: Vanilla k8s Cluster](#extra-vanilla-k8s-cluster)
- [IDL containerised RUCIO client - User Guide](#idl-containerised-rucio-client---user-guide)

## Preliminaries

_**NOTE**: copy the kubeconfig file generated by the ICSC Cloud infrastructure at the start of the k8s cluster to your working machine and run `export KUBECONFIG=/path/to/your/kubeconfig.txt` for every new terminal window, otherwise add it in the .bashrc file (could cause safety issues if some users in the working machine are not allowed to tinker with the k8s cluster)._

- Git clone the Spoke2 IDL repository: https://github.com/ICSC-Spoke2-repo/idl-rucio.git 

- Install kubectl: https://kubernetes.io/docs/tasks/tools/install-kubectl/

- Install helm: https://helm.sh/docs/intro/install/

Add Helm chart repositories:

```bash
- helm repo add stable https://charts.helm.sh/stable
- helm repo add bitnami https://charts.bitnami.com/bitnami
- helm repo add rucio https://rucio.github.io/helm-charts
```

Install a dynamic provisioner (`local-path provisioner` in our case): https://github.com/rancher/local-path-provisioner?tab=readme-ov-file#installation

Make it the default `storageClass`:

```bash
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

Create a working namespace in the k8s cluster:

```bash
kubectl create ns rucio-idl
```

Set it the default namespace in the kubeconfig (keep it in mind if you aren’t the only one working on this k8s cluster):

```bash
kubectl config set-context --current --namespace=rucio-idl
```

## RUCIO Server Deployment

- `cd` into the git cloned repository

- [Start up the DB](server/manifests/postgres.yaml) for the RUCIO server: 

```bash
helm install rucio-db bitnami/postgresql -f server/manifests/postgres.yaml 
```

> "rucio-db" is the helm release name

- [Initialize](server/manifests/rucio-init.yaml) the DB:

```bash
kubectl apply -f server/manifests/rucio-init.yaml
```

- Create the secrets with names as \<SERVER_HELM_RELEASE_NAME\>-...:
  - The `permissions.py` file is the generic one where I changed `perm_get_signed_url`, to allow all users to write in the RSEs, and `perm_add_replicas`, so that non-admin users are allowed to upload files only for their scope (scopes must be in the format: user.\<ACCOUNT\>)
    ```bash
    kubectl create secret generic server-permissions --from-file=/path/to/server/policy-package/permissions.py
    
    kubectl create secret generic server-plugin --from-file=/path/to/server/meta-db/custom-did-meta-plugin.py
    
    kubectl create secret generic server-ayradb-cluster --from-file=/path/to/server/secrets/metaDB_credentials_template.cfg
    ```
  - This secret is needed to allow the server to authenticate to an [S3 RSE](#rse-configuration)
    ```bash
    kubectl create secret generic server-rse-accounts --from-file=/path/to/server/secrets/rse-accounts-template.cfg
    ```

- Deploy the [RUCIO daemons](server/manifests/daemons-rucio.yaml) (“helm install” is just for the first time, after that use helm upgrade):

```bash
helm install daemons rucio/rucio-daemons -f server/manifests/daemons-rucio.yaml
```

> "daemons" is the helm release name

- Create the rse-accounts secret for the daemons helm release, so that they are allowed to delete files from S3 storages:

```bash
kubectl create secret generic daemons-rse-accounts --from-file=/path/to/server/secrets/rse-accounts-template.cfg
```

- Unpack the custom RUCIO server [Helm chart](helm-rucio-server.tgz). This helm chart is needed to mount the `pvc` backed by NFS in the server replicas.

- Deploy the [RUCIO server](server/manifests/rucio-server.yaml) (“helm install” is just for the first time, after that use helm upgrade) [IF YOU ARE ON A VANILLA K8S CLUSTER FOLLOW Setting up a Rucio server on Kubernetes in a cluster]:

```bash
helm install server ./helm-rucio-server -f server/manifests/rucio-server.yaml 
```

> "server" is the helm release name

- Deploy a [root RUCIO client](client-test/manifests/root-rucio-client.yaml) (needed for special actions if you don't update the policies):

```bash
kubectl apply -f client-test/manifests/root-rucio-client.yaml
```

## S3 Storage Endpoint

### RSE Setup

- Follow the README in the [minio](minio/) folder. 

- Make sure that the traffic to the ports in the MinIO tutorial is allowed.

### RSE Configuration

From the root client, create a RUCIO Storage Element (RSE):

```bash
rucio-admin rse add <RSE_NAME>
```

Add a protocol to the RSE:

```bash
rucio-admin rse add-protocol --hostname <YOUR_S3_HOSTNAME>  --domain-json '{"lan": {"write": 1, "read": 1, "delete": 1}, "wan": {"write": 1, "read": 1, "delete": 1, "third_party_copy_read": 1, "third_party_copy_write": 1}}' --impl rucio.rse.protocols.gfal.NoRename --scheme https --prefix <BUCKET_NAME> --port 443 <RSE_NAME>
```

Set a non-zero quota for the RSE:

```bash
rucio-admin account set-limits root <RSE_NAME> <QUOTA>GB`
```

> change the \<QUOTA\> with what you need; `"-1"` is for no limits.

Set the following attributes:

```bash
rucio-admin rse set-attribute --rse <RSE_NAME> --key sign_url --value s3
rucio-admin rse set-attribute --rse <RSE_NAME> --key skip_upload_stat --value True
rucio-admin rse set-attribute --rse <RSE_NAME> --key verify_checksum --value False
rucio-admin rse set-attribute --rse <RSE_NAME> --key strict_copy --value True
rucio-admin rse set-attribute --rse <RSE_NAME> --key s3_url_style --value path
```

Create a `rse-accounts.cfg` (use your `rse_id` and s3 credentials below):

```bash
cat >> etc/rse-accounts.cfg << EOL
{
    “YOUR_RSE_ID”: {
        “access_key”: “<YOUR_ACCESS_KEY>”,
        “secret_key”: “<YOUR_SECRET_KEY>”,
        “signature_version”: “s3v4”
        “region”: “us-west-2”
    }
}
EOL
```

Give every Rucio account, including root, the following attribute to be able to sign URLs:

```bash
rucio-admin account add-attribute <ACCOUNT_NAME> --key sign-gcs --value true
```

### RSE Test

Create a test file:

```bash
dd if=/dev/urandom of=mydata bs=10M count=1
```

To upload the file, an account must have a scope. Create a scope linked to your account:

```bash
rucio-admin scope add --account root --scope test
```

Upload the file in the RSE in the test scope:

```bash
rucio upload --scope test --rse <RSE_NAME> mydata
```

Verify that your file is in your MinIO instance (e.g. `mc ls myminio/test/test/8e/d4`):

```bash
mc ls myminio/test/path/to/uploaded/file
```

Download the file:

```bash
rucio download test:mydata
```

## IDL-dedicated components

### K8s Cronjobs: Periodic Dump

The dump method is responsible for periodically exporting the contents of specific tables in AyraDB to an internal warehouse to perform fast SQL queries. In a k8s environment, this method is run automatically via a k8s Cronjob, which periodically launches a container configured to dump the tables and log the outcome of the operation.

To trigger [this k8s Cronjob](server/manifests/cronjob-dump-sanity.yaml) run:

```bash
kubectl apply -f server/manifests/cronjob-dump-sanity.yaml
```

The cluster admins can also [deploy a pod](server/manifests/pod-dump-sanity.yaml) with the same Docker image used to configure the periodic dump, to run dumps manually:

```bash
kubectl apply -f server/manifests/pod-dump-sanity.yaml
```

by entering the pod:

```bash
kubectl exec -it manual-dump-and-sanity-check -- bash
```

and run (the other two flags are `--tasi` and `--inaf` to run the dump only on those tables):

```bash
idl_dump.py --all 
```

### K8s Cronjobs: Consistency-check

The consistency-check method analyzes consistency between metadata entries in the internal RUCIO DB and the external DB. It identifies three main groups:
- Hanging metadata (external DB records with no corresponding replicas in RUCIO)
- Orphan replicas (RUCIO replicas with no metadata in the external DB)
- Intersection (valid DIDs both in the external DB and in RUCIO)

To handle large number of records (the project expects billions of records in production), the consistency-check implementation adopts:
- **Pagination**: Metadata entries and replica data are retrieved in batches using an index-based pagination approach. This avoids overloading memory and APIs when scanning millions of DIDs.
- **Multithreading**: Each pagination chunk is processed concurrently using worker threads. This parallel approach significantly speeds up the consistency scan and improves throughput.

> This is a working prototype so it must be further optimized and fine-tuned to be used in production

This method is also periodically run using a k8s Cronjob. It can be configured to:
- only print the number of findings (and save them in local files) without modifying anything
- declare orphan replicas as bad in the RSEs (`--declare-bad`)
- delete hanging metadata (`--delete`)

To trigger [this k8s Cronjob](server/manifests/cronjob-dump-sanity.yaml) run:
```bash
kubectl apply -f server/manifests/cronjob-dump-sanity.yaml
```

> Work in Progress: After a replica has been declared bad you can run the daemon `rucio-necromancer` from a server replica:
>  ```bash
>  rucio-necromancer --run-once
>  ```
>  to turn the bad replicas in the `LOST` state in RUCIO and then manually erase those DIDs and, if it shouldn't have been a bad replica, try to re-upload it.

The cluster admins can also [deploy a pod](server/manifests/pod-dump-sanity.yaml) with the same Docker image used to configure the periodic consistency-check, to run it manually:

```bash
kubectl apply -f server/manifests/pod-dump-sanity.yaml
```

by entering the pod:

```bash
kubectl exec -it manual-dump-and-sanity-check -- bash
```

and run (the two available flags are `--delete` and `--declare-bad` to run the dump only on those tables):

```bash
idl_consistency_check.py
```

### Shared Storage: NFS

#### NFS Server Setup

- Follow the README in the [nfs](nfs/) folder. 

- Make sure that the traffic to the ports in the NFS tutorial is allowed.

#### RUCIO Server's PVC

Create a [Persistent Volume](nfs/manifests/pv-nfs.yaml) backed by the NFS server:

```bash
kubectl apply -f nfs/manifests/pv-nfs.yaml
```

Claim it with a [PVC](nfs/manifests/pvc-nfs.yaml):

```bash
kubectl apply -f nfs/manifests/pvc-nfs.yaml
```

> To mount a PVC in the replicas of the RUCIO server, you need to use the [custom Helm chart](helm-rucio-server.tgz) provided in the IDL Spoke2 GitHub repository. 

Make sure the [rucio-server.yaml](server/manifests/rucio-server.yaml) has:

```bash
nfs:
  pvcName: <PVC_NAME>
```

restart the deployment if not.

## Next-Gen RUCIO WebUI

- Deploy the [RUCIO WebUI](webui/values-ruciowebui.yaml) (helm upgrade after the first time) [IF YOU ARE ON A VANILLA K8S CLUSTER YOU MUST HAVE FOLLOWED Setting up a Rucio server on Kubernetes in a cluster and use the name of the `ClusterIssuer` resource for the `ingress.annotations.cert-manager.io/cluster-issuer` value]:

  ```bash
  helm install webui rucio/rucio-webui -f webui/values-ruciowebui.yaml
  ```

  > "webui" is the helm release name

- Wait for the WebUI to start up correctly (might take a few minutes). After the WebUI pod enters the Running state, you can follow the start up with:

  ```bash
  kubectl logs -f <WEBUI_POD> -c rucio-webui
  ```

## JupyterHub instance

- Add Helm chart repository:

  ```bash
  helm repo add jupyterhub https://hub.jupyter.org/helm-chart/
  ```

- Run:

  ```bash
  helm repo update
  ```

- Deploy the [JupyterHub](jupyterhub/jhub/jupyter-config.yaml) instance with the rucio-jupyterlab extension [IF YOU ARE ON A VANILLA K8S CLUSTER YOU MUST HAVE FOLLOWED Setting up a Rucio server on Kubernetes in a cluster and use the name of the `ClusterIssuer` resource for the `ingress.annotations.cert-manager.io/cluster-issuer` value]:

  ```bash
  helm install --cleanup-on-fail --install jupyterhub jupyterhub/jupyterhub --namespace jupyterhub-idl --create-namespace --version=<chart-version> --values jupyterhub/jhub/jupyter-config.yaml
  ```

  > "jupyterhub" is the helm release name

- Wait for the hub and proxy pod to enter the Running state:

  ```bash
  kubectl get pod -n jupyterhub-idl 
  ```

- To verify that JupyterHub is working, enter `https://<JUPYTERHUB_HOSTNAME>:443` in a browser.

- If you use the custom docker image lucapacioselli/jupyter4rucio:\<TAG\> there is already the rucio-jupyterlab extension installed and the rucio-client installation with the custom IDL client. To use the rucio-jlab extension you must insert your credentials in the Configuration section of the extension that appears on the left sidebar.

> the rucio-jupyterlab extension doesn't work with RUCIO server versions greater than 35.2.0, because the `did_type` formatting changed. This issue should be solved with rucio-jupyterlab extension version 1.3.0

- After this, you should be able to search for every file in the RSE in the Explore section and make it available to the current JLab session (this will create a sub-folder in the /home/jovyan/rucio/ directory). If you also want to use a file made available from rucio in a notebook, you need to select a notebook tab and click the Add to Notebook button to attach it to the current notebook (you can also choose a variable name for the file). The Notebook section has the list of attached files to the current selected notebook tab. You can detach a file from your current notebook through the usual red cross.

- If you want to use the Python API Client in the jupyter notebook run the following in the first cell:

  ```bash
  import os
  os.environ['RUCIO_HOME']="/path/to/the/rucio.cfg"
  ```

## Extra: Vanilla k8s Cluster

_**Skip this section if you are using an "ICSC Cloud k8s cluster"**_

Install the `nginx-ingress` Helm chart as a DaemonSet:

```bash
helm upgrade --install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --namespace ingress-nginx --create-namespace --set controller.kind=DaemonSet
```

Patch the nginx DaemonSet: 

```bash
kubectl edit daemonset ingress-nginx-controller -n ingress-nginx
```

Add the following under “spec.template.spec” (so that the nginx-controller pod is on the master node of the cluster): 

```bash
affinity:
       nodeAffinity:
         requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      volumes:
      - name: webhook-cert
        secret:
          defaultMode: 420
          secretName: ingress-nginx-admission
```

Follow the "Via the host network" section of the bare-metal considerations guide: https://kubernetes.github.io/ingress-nginx/deploy/baremetal/

Install the default static configuration of the `cert-manager`:

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.15.1/cert-manager.yaml
```

Write the following ClusterIssuer resource (cert-issuer.yaml):

```bash
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
 name: lets-issuer
 namespace: cert-manager
spec:
 acme:
   # The ACME server URL
   server: https://acme-v02.api.letsencrypt.org/directory
   # Email address used for ACME registration
   email: <YOUR_EMAIL>
   # Name of a secret used to store the ACME account private key
   privateKeySecretRef:
     name: letsencrypt
   # Enable the HTTP-01 challenge provider
   solvers:
   - http01:
       ingress:
         class: nginx
```

Create the cert-issuer:

```bash
kubectl apply -f /path/to/the/cert-issuer.yaml
```

Follow the "Troubleshooting" guide to verify that the certificate-related resources are working fine: https://cert-manager.io/docs/troubleshooting/

Update the rucio-server.yaml ingress value to match the following:

```bash
ingress:
 enabled: true
 annotations:
   nginx.ingress.kubernetes.io/ssl-passthrough: "false"
   nginx.ingress.kubernetes.io/ssl-redirect: "false"
   cert-manager.io/cluster-issuer: lets-issuer # this must match the name of the ClusterIssuer resource created before
 ingressClassName: nginx
 hosts:
   - <SERVER_HOSTNAME>
 tls:
 - hosts:
   - <SERVER_HOSTNAME>
   secretName: rucio-idl-server-tls # change if needed
```

Restart the server:

```bash
helm upgrade server ./helm-rucio-server -f /server/manifests/rucio-server.yaml
kubectl delete pod <RUCIO_SERVER_POD> # run this if after the helm upgrade the pod hasn't restarted
```

## IDL containerised RUCIO client - User Guide

This guide explains how to install, configure, and use the Rucio client inside a Docker container with IDL-specific extensions for file upload, metadata management, and external database queries.

_**NOTE**: To use RUCIO's primitives that share the same name as IDL's methods (e.g. list-dids, get-metadata) using the IDL script you must add the flag `--rucio` in the command_

### Prerequisites

Docker must be installed on your system: https://docs.docker.com/engine/install/


### 1. Setting Up the Container

Pull the custom Docker image:

```bash
docker pull lucapacioselli/rucio-client-test:latest
```

Run the container:

```bash
docker run --name=<YOUR_CONTAINER_NAME> -it -d lucapacioselli/rucio-client-test:latest
```

Enter the container (after it has been started):

```bash
docker exec -it <YOUR_CONTAINER_NAME> /bin/bash
```

### 2. Configuration

Inside the container, edit the rucio.cfg configuration file directly or using the cred.py script:

```bash
cred.py --user <USERNAME> --account <ACCOUNT>
```

The script will prompt for your password:

```bash
Enter the password (hidden):
```

### 3. Data Lake Injection: Uploading Files and Metadata

Upload a single Data file with its Metadata

- customUpload method: upload data file in the Data Lake (RUCIO) + set-metadata from the metadata .json file in the external DB (AyraDB):
  ```bash
  IDL upload --scope <SCOPE> --rse IDL_TEST --file <DATA_FILE> --meta <METADATA_JSON>
  ```

Upload multiple files and metadata from the terminal

- Ensure the order of data files and metadata files matches (i.e., meta1 is assigned to data1, meta2 to data2, etc.):
  ```bash
  IDL upload --scope <SCOPE> --rse IDL_TEST --file <DATA1> <DATA2> ... --meta <META_JSON1> <META_JSON2> ...
  ```

Bulk upload from a text file

- Create a .txt file where each line contains the path to a data file and its corresponding metadata file, separated by a space:
  ```bash
  /path/to/data1 /path/to/meta1
  /path/to/data2 /path/to/meta2
  ```
- Run:
  ```bash
  IDL upload --scope <SCOPE> --rse IDL_TEST --bulk-txt <TXT_FILE> 
  ```

### 4. Retrieving Metadata

Custom get-metadata to retrieve metadata from AyraDB for one or more DIDs:

```bash
IDL get-metadata <DID_SCOPE>:<DID1_NAME> ... <DID_SCOPE>:<DIDn_NAME>
```

> All DIDs in one call must belong to the same scope.

### 5. Database Queries

Filter DIDs from AyraDB records:

```bash
IDL list-dids --scope <SCOPE> --filters <FILTERS>
```

Execute a custom SQL query to AyraDB using metadata filters:

```bash
IDL sql --scope <SCOPE> --select <SELECT> --filters <FILTERS>
```

- **\<SELECT\>** and **\<FILTERS\>** depend on the metadata schema being queried (i.e. TASI table or INAF tables, which depends by the scope: default is the TASI schema; the INAF schema is used if the scope is "fermi", "birales", or "pulsar")
- **\<SELECT\>**: Specifies the columns to retrieve in an SQL query.
Example: `--select 'PARTICIPANT_1, EPOCH'`
The LINK field (representing the file's DID) is always included automatically. Wildcards such as * are supported.
- **\<FILTERS\>**: Defines conditions for filtering metadata.
Operators allowed: `<=, >=, =, !=, >, <`
Logical operators AND and OR must be used to build the query.
Example: `--filters "EPOCH >= 2021 AND PARTICIPANT_1 != 'None'"`

### 6. Account management:

Retrieve info on the current user:

```bash
IDL whoami
```

Show account usage

```bash
IDL list-account-usage <ACCOUNT>
```

List the rules defined for a given account

```bash
IDL list-rules --account <ACCOUNT>
```

### 7. Rucio Storage Element (RSE) Management:

_**NOTE**: For testing, you can also use the RSE "TESTBED_USERDISK"._

List available RSEs

```bash
IDL list-rses
```

Show RSE usage

```bash
IDL list-rse-usage IDL_TEST
```

List datasets stored on a specific RSE

```bash
IDL list-datasets-rse IDL_TEST
```

### 8. Scopes:

List all scopes:

```bash
IDL list-scopes
```

### 9. DID Management:

Create a container (collection of containers and/or datasets):

```bash
IDL add-container <SCOPE>:<container>
```

Create a dataset (collection of files):

```bash
IDL add-dataset <SCOPE>:<dataset>
```

Download a file (or an entire dataset):

```bash
IDL download <SCOPE>:<DID_NAME>
```

Add a file to an existing dataset:

```bash
IDL attach <SCOPE>:<dataset> <SCOPE>:<DATA_FILE>
```
Get the datasets in a RSE:

```bash
IDL list-datasets-rse IDL_TEST
```

Get the files in a dataset:

```bash
IDL list-files <SCOPE>:<dataset>
```

Find where files are stored

```bash
IDL list-file-replicas <SCOPE>:<DID_NAME>
```

Get file/dataset/container attributes

```bash
IDL stat <SCOPE>:<DID_NAME>
```
